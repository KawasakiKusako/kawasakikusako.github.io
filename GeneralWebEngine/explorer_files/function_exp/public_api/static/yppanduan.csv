题号,题目,答案
1,Python 读取 word 文档（.docx）时，需要安装 python-docx 库。,True
2,jieba 分词的精确模式会列出所有可能的分词结果，适合搜索引擎构建索引。,False
3,停用词在所有 NLP 任务中都需要去除，以提高处理效率。,False
4,Ollama 本地部署大模型后，再次启动时无需重新拉取模型。,True
5,调用大模型 API 时，将 API Key 直接写入代码比放入环境变量更安全。,False
6,情感分析任务中，标签为 1 代表正面情感，-1 代表负面情感，这是唯一的标注方式。,False
7,词袋模型中，“人咬狗” 和 “狗咬人” 会被处理为相同的向量。,True
8,TF-IDF 方法同时考虑了词在文档中的频率和在语料库中的分布。,True
9,信息增益值越低，说明该特征对分类任务的贡献越大。,False
10,模型训练完成后，测试集的准确率越高，模型的泛化能力一定越强。,False
11,TextRank 算法的初始权重应设置为不同值，以保证收敛速度。,False
12,词云图的可视化效果仅与词频有关，与词的语义无关。,True
13,LDA 模型是一种监督学习模型，需要人工标注主题标签。,False
14,困惑度越小，说明 LDA 模型对文本的预测能力越强。,True
15,LangChain 的检索增强技术仅适用于本地文档的问答任务。,False
16,RAG 技术中，相似度计算的核心是比较问题向量与文档片段向量的距离。,True
17,K-means 算法对初始聚类中心的选择不敏感。,False
18,DBSCAN 算法能够很好地处理非球状分布的簇。,True
19,word2vec 的 CBOW 模型是根据中心词预测周围词。,False
20,FastText 词向量仅支持英文文本的向量化。,False
21,BERT 模型的自注意力机制能够捕捉词与词之间的长距离依赖关系。,True
22,使用 Hugging Face Transformers 库时，必须安装 PyTorch 或 TensorFlow。,True
23,SentenceTransformer 库提取的句子向量维度固定为 768 维。,False
24,BERTopic 相比 LDA，文本向量化时考虑了上下文语义。,True
25,BERTopic 默认使用的降维方法是 PCA。,False
26,LangChain 的 SimpleSequentialChain 支持多输入多输出的复杂任务。,False
27,异步链适用于后一次调用依赖前一次结果的场景。,False
28,路由链可以根据用户问题类型自动选择对应的处理链。,True
29,摘要链处理长文本时，无需考虑大模型的输入 token 上限。,False
30,中文文本分词后，需要用空格连接才能被 sklearn 的 CountVectorizer 处理。,True
31,命名实体识别（NER）任务中，“O” 标签表示该位置不属于任何实体。,True
32,机器翻译任务的文本标注需要构建源语言与目标语言的平行语料。,True
33,数据预处理中，词性标注是所有 NLP 任务的必要步骤。,False
34,传统机器学习的文本表示中，一条文本对应一个向量。,True
35,大模型的 “涌现现象” 是指参数量达到一定规模后，模型能力突然提升。,True
36,提示词注入是一种通过优化输入提升模型性能的合法技术。,False
37,方面级情感分析可以同时识别文本中的多个属性及其情感倾向。,True
38,多属性决策中，综合评价值的计算必须采用加权求和的方式。,False
39,BERTopic 的聚类结果中，离群值会被分配到某个簇中。,False
40,LangChain 的 Memory Module 主要用于存储模型的配置信息。,False